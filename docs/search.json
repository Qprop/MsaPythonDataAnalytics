[
  {
    "objectID": "MsaPythonDataAnalytics.html#welcome",
    "href": "MsaPythonDataAnalytics.html#welcome",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Welcome",
    "text": "Welcome\nWe are embarking on a journey, with multiple stops but the destination will be far out. The stops will be areas of expertise and learning that we will have done and practiced but the destination is the progressive accumulation of the learning.\nWith the ever evolving changing of technology the destination will always be changing."
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#data-analytics",
    "href": "MsaPythonDataAnalytics.html#data-analytics",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Data Analytics",
    "text": "Data Analytics\nColloquial term data analytics can be coined as both a science and an art. With the science part majorly following statistical/mathematical procedures used, art comes from the different ways and methods someone can use to present and execute the since part of it.\nTools for Data Analytics\nThere are various tools/Software/applications can be used for data analysis\n\n\n\nPaid for:\n\nMs Excel\nMs Power BI\nSPSS\nSTATA\nSAS\nMS SQL\n\n\n\n\nOpen Source: Free\n\nPython\nR\nPostgres\nJulia\nMangoDB\nCouchDB"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-introduction",
    "href": "MsaPythonDataAnalytics.html#python-introduction",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python Introduction",
    "text": "Python Introduction\nOpen Source Programming language.First come to use in the early 1990’s and developed by Guido Van Rossum more information here.\n\n\nAdvantages\n\nEasy to learn\nAlmost language like syntax\nFast execution\nAll Purpose programming language:\n\nused for software development\nused for data analysis\nused for machine learning\nused for web development\n\n\n\nDisadvantages\n\nSome convections are different from other programming language\nHigh level interpreted language"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-interface",
    "href": "MsaPythonDataAnalytics.html#python-interface",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python Interface",
    "text": "Python Interface\nPython programming language uses it’s in build command prompt frequently called CLI standing for Command Line Interface, search through windows/MAC program files and you should see Python 3.0.0) there are various versions of python and depending on which one you have installed this will determine the python CLI.\nThough you can be able to do everything using this, it doesn’t give an intuitive user interface hence the reason for development of IDE Integrated Development Environment.\nIDE is the dashboard similar to car dashboard but the actual software the engine is now python for this case.\n\nIDE’s Common to PythonInstallations\n\n\n\nPycharm\nSpyder\njupyter\nvsCode\nRstudio\nPositron (New in Beta)\nText editors\n\nnotepad ++\nvim\nsublime\n\ne.t.c\n\nThough there are many and the tool of choice is open for use, for now we can focus on using vsCode as this is universal to also other programming languages but if you are interested in a polygot system you can test positron.\n\n\nLet us check what installations you have\n\nPython Installation\nIDE Installation\nTo install all this together we prefer anaconda\n\nAnaconda | full suit of packages and tools\nminiconda | minimal and necessary packages"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-hello-world",
    "href": "MsaPythonDataAnalytics.html#python-hello-world",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python | Hello World",
    "text": "Python | Hello World\nPython being an interpreted high level programming language, making things easier for the programmer. Able to pick up the things very easily.\nLet us start with the first code.\n\nHello WorldKeywordsData typesMethods & FunctionsSummary Sheets\n\n\n\n\nCode\nprint(\"Hello WOrld\")\n\n\nHello WOrld\n\n\nLet us use it as a calculator.\n\n\nCode\n2 + 2  #Add\n#4\n3 - 1  #Subtract\n#2\n4 * 5  #Multiply\n#20\n20 / 5 #Divide\n#4 \n5 ** 2 #Exponent\n#25\n5 % 2  #?What is the result\n\n\nUsing it as an input\n\n\nCode\ninput(\"What's your name\")\n\n\nUsing comments\n\n\nCode\n#This is a comment\n\n\nAssigning objects to names variables\n\n\nCode\ndepartment = \"DT\"\n\nprint(department)\n\n# you can change the variable on the fly\n\ndepartment = 'Customs'\n\nprint(department)\n\n\nDT\nCustoms\n\n\nRules for variable names\n\nCan’t start with numbers (1,2,3,4….)\nLetter, numbers, underscores are allowed in the name but ’,-, spaces are not allowed\n\n\n\npython has keywords this are words that have syntactical use in the program below list even though not fully conclusive.\nand continue except global lambda raise yield\nas def exec if not return\nassert del finally import or try\nbreak elif for in pass while\nclass else from is print with\n\n\nNumbers\nIntegers, floating point numbers and complex numbers falls under Python numbers category.\n\nWe can use the type() function to know which class a variable or a value belongs to and the isinstance() function to check if an object belongs to a particular class.\n\n\n\nCode\na = 5\nprint(a, \"is of type\", type(a))\n\na = 2.0\nprint(a, \"is of type\", type(a))\n\na = 1+2j\nprint(a, \"is of type\", type(a))\nprint(a, \"is complex number?\", isinstance(1+2j,complex))\n\n\n5 is of type &lt;class 'int'&gt;\n2.0 is of type &lt;class 'float'&gt;\n(1+2j) is of type &lt;class 'complex'&gt;\n(1+2j) is complex number? True\n\n\nStrings\nsequence of characters used to store and represent text-based information\n\n\nCode\nfirst_string = \"My first String\"\nfirst_string\n\n\n'My first String'\n\n\n\n\nCode\nlong_string = \"\"\"Very long string\nspanning multiple lines\nthat never seem to come to an end\"\"\"\nlong_string\n\n\n\n\nCode\nprint(first_string.capitalize())\nprint(first_string.title())\nprint(first_string.upper())\nprint(first_string.swapcase())\nprint(first_string.find('is'))\nprint(first_string.replace('first', 'second'))\nprint(first_string.strip())\nprint(first_string.isalnum())\nprint(first_string.isalpha())\nprint(first_string.isdigit())\nprint(first_string.isprintable())\n\n\nList\nMutable ordered sequence of items.\n\n\nCode\nfirst_list = [1,2,3,4,1,1,1,1]\nfirst_list\n\n\n[1, 2, 3, 4, 1, 1, 1, 1]\n\n\nList objects provide several methods\n\n\nCode\nfirst_list.count(1)\nfirst_list.index(1)\nfirst_list.append(5)\nfirst_list.remove(5)\nfirst_list.pop(-1)\nfirst_list.reverse()\nfirst_list.sort()\n\n\nTuples\nImmutable ordered sequence of items.Tuples once created cannot be modified.\n\n\nCode\nfirst_tuple = (1,2,3)\nfirst_tuple\n\n\n(1, 2, 3)\n\n\nSets\nOrdered collections of unique items.\n\n\nCode\n{42, 3.14, 'hello'} # Literal for a set with three items\n{100} # Literal for a set with one item\nset() # Empty set (can't use {}—empty dict!)\n\n\nset()\n\n\nDictionary\nArbitrary collection of objects indexed by nearly arbitrary values called keys.\n\n\nCode\nfirst_dic = {'a' : [1,2,3], 'b' : [4,5,6], 'c' : [7,6,8]}\nfirst_dic\n\n\n{'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 6, 8]}\n\n\n\n\nCode\n{'x':42, 'y':3.14, 'z':7} # Dictionary with three items, str keys\n{1:2, 3:4} # Dictionary with two items, int keys\n{1:'za', 'br':23} # Dictionary with mixed key types\n{} # Empty dictionary\n\n\ndict(x=42, y=3.14, z=7) # Dictionary with three items, str keys\ndict([(1, 2), (3, 4)]) # Dictionary with two items, int keys\ndict([(1,'za'), ('br',23)]) # Dictionary with mixed key types\ndict() # Empty dictionary\n\n\n\n\nMethods\nMethod: Attributes associated to different objects and data types. As well classes at a broader level\n\n\nCode\nfirst_string.upper()\nfirst_string.lower()\nfirst_string.swapcase()\nfirst_string.rsplit() #separating or delimiter is a space\n\n\n['My', 'first', 'String']\n\n\nFunctions\n\n\nCode\nfirst_list = [1,2,3,4]\nfirst_list\n\n\n[1, 2, 3, 4]\n\n\n\n\nassociativity of the operator: L (left-to-right), R (right-to-left), or NA (nonassociative).\n\n\n\n\n\n\n\n\nOperator\nDescription\nAssociativity\n\n\n\n\n{key:expr,...}\nDictionary creation\nNA\n\n\n{ expr ,...}\nSet creation\nNA\n\n\n[ expr ,...]\nList creation\nNA\n\n\n( expr ,...)\nTuple creation or just parentheses\nNA\n\n\nf ( expr ,...)\nFunction call\nL\n\n\nx [ index : index ]\nSlicing\nL\n\n\nx [ index ]\nIndexing\nL\n\n\nx . attr\nAttribute reference\nL\n\n\nx ** y\nExponentiation (x to the yth power)\nR\n\n\n~ x\nBitwise NOT\nNA\n\n\n+x, -x\nUnary plus and minus\nNA\n\n\nx*y, x/y, x//y, x%y\nMultiplication, division, truncating division,remainder\nL\n\n\nx+y, x-y\nAddition, subtraction\nL\n\n\nx&lt;&lt;y, x&gt;&gt;y\nLeft-shift, right-shift\nL\n\n\nx & y\nBitwise AND\nL\n\n\nx ^ y\nBitwise XOR\nL\n\n\nx | y\nBitwise OR\nL\n\n\nx&lt;y, x&lt;=y, x&gt;y, x&gt;=y, x&lt;&gt;y (v2 only),x!=y, x==y\nComparisons (less than, less than or equal, greater than, greater than or equal, inequality, equality)a\nNA\n\n\nx is y, x is not y\nIdentity tests\nNA\n\n\nx in y, x not in y\nMembership tests\nNA\n\n\nnot x\nBoolean NOT\nNA\n\n\nx and y\nBoolean AND\nL\n\n\nx or y\nBoolean OR\nL\n\n\nx if expr else y\nTernary operator\nNA\n\n\nlambda arg,...: expr\nAnonymous simple function\nNA"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-further-into-hello-world",
    "href": "MsaPythonDataAnalytics.html#python-further-into-hello-world",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python | Further into Hello World",
    "text": "Python | Further into Hello World\n\nConditions & Iterations\n\n\nConditions criteria where we compare values and decide what step to take. Example of conditional criteria if-else , if-elif-else, while.\n\n\nCode\nif condition:\n  #do something\nelse:\n  #do something\n\n\nComparison operators go hand in hand with conditions. Comparison operators inculde == , &lt;=, &gt;=, |, &, or, and e.t.c\nIterations is repeating and the most common form of iteration is for Code highligt for for\n\n\nCode\nfor value in a_list:\n  #do something"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-modules",
    "href": "MsaPythonDataAnalytics.html#python-modules",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python modules",
    "text": "Python modules"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#modules-introduction",
    "href": "MsaPythonDataAnalytics.html#modules-introduction",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Modules introduction",
    "text": "Modules introduction\nWhen our program grows bigger, it is a good idea to break it into different modules.\nA module is a file containing Python definitions and statements. Python modules have a filename and end with the extension .py.\nDefinitions inside a module can be imported to another module or the interactive interpreter in Python. We use the import keyword to do this.\nFor example, we can import the math module by typing in import math.\n\n\nCode\nimport math\nprint(math.pi)\n\n\n3.141592653589793\n\n\nChecking paths using sys module\n\n\nCode\nimport sys\nprint(sys.path)\n\n\n['C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', '', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\Pythonwin']"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-numpy-introduction",
    "href": "MsaPythonDataAnalytics.html#python-numpy-introduction",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python numpy | Introduction",
    "text": "Python numpy | Introduction\nnumpy: python module/library specialized in Arrays and Vectorized Computation.\nNumPy, short for Numerical Python, is one of the most important foundational packages for numerical computing in Python. Numpy works with array from 1-n dimensional.\n\nIntroArray descriptors & inspectorsArray ArithmeticsArray Manipulation\n\n\n\n\nCode\nimport numpy as np\n\nmy_arr = np.arange(10)\n\n#my_list = list(range(10)) #inbuilt python\n\nmy_arr\n\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nMultidimensional Array\n\n\nCode\ndata = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])\n\ndata\n\na = np.array([1,2,3])\nb = np.array([(1.5,2,3), (4,5,6)], dtype = float)\nc = np.array([[(1.5,2,3), (4,5,6)],[(3,2,1), (4,5,6)]], dtype = float)\n\nnp.zeros((3,4)) #Create an array of zeros\nnp.ones((2,3,4),dtype=np.int16) #Create an array of ones\nd = np.arange(10,25,5)#Create an array of evenly spaced values (step value)\nnp.linspace(0,2,9) #Create an array of evenlyspaced values (number of samples)\ne = np.full((2,2),7)#Create a constant array\nf = np.eye(2) #Create a 2X2 identity matrix\nnp.random.random((2,2)) #Create an array with random values\nnp.empty((3,2)) #Create an empty array\n\n\narray([[1.39069238e-309, 1.39069238e-309],\n       [1.39069238e-309, 1.39069238e-309],\n       [1.39069238e-309, 1.39069238e-309]])\n\n\n\n\n\n\nCode\ndata.shape #Array dimensions\nlen(a)#Length of array\nb.ndim #Number of array dimensions\ne.size #Number of array elements\nb.dtype  #Data type of array elements\nb.dtype.name  #Name of data type\nb.astype(int). #Convert an array to a different type\n\n\n\n\n\n\nCode\ng = a - b. #Subtraction\n\nnp.subtract(a,b) #Subtraction\nb + a #Addition \n\nnp.add(b,a) #Addition \na/b #Division \n\nnp.divide(a,b) #Division \na * b #Multiplication \n\nnp.multiply(a,b) #Multiplication \nnp.exp(b) #Exponentiation\nnp.sqrt(b) #Square root\nnp.sin(a)  #Print sines of an array\nnp.cos(b) #Elementwise cosine\nnp.log(a)#Elementwise natural logarithm\ne.dot(f) #Dot product \n\n\nComparison\n\n\nCode\na == b #Elementwise comparison\n\na&lt; 2 #Elementwise comparison\n\nnp.array_equal(a, b) #Arraywise comparison\n\n\nSorting Arrays\n\n\nCode\na.sort() #Sort an array\nc.sort(axis=0) #Sort the elements of an array's axis\n\n\n**Subsetting, slicing, indexing\n\n\nCode\na[2] #Select the element at the 2nd index\n\nb[1,2] #Select the element at row 1 column 2(equivalent to b[1][2])\n\n\na[0:2]#Select items at index 0 and 1\n\nb[0:2,1] #Select items at rows 0 and 1 in column 1\n\nb[:1] #Select all items at row0(equivalent to b[0:1, :])\n\nc[1,...] #Same as[1,:,:]\n\na[ : : -1] #Reversed array a array([3, 2, 1])\n\na[a&lt;2] #Select elements from a less than 2\n\nb[[1,0,1, 0],[0,1, 2, 0]] #Select elements(1,0),(0,1),(1,2) and(0,0)\n\nb[[1,0,1, 0]][:,[0,1,2,0]] #Select a subset of the matrix’s rows and columns\n\n\n\n\n\n\nCode\ni = np.transpose(b) #Permute array dimensions\ni.T #Permute array dimensions\n\nb.ravel() #Flatten the array\ng.reshape(3, -2) #Reshape, but don’t change data\n\nh.resize((2,6)) #Return a new arraywith shape(2,6)\nnp.append(h,g) #Append items to an array\nnp.insert(a,1,5)  #Insert items in an array\nnp.delete(a,[1])  #Delete items from an array\n\n\nnp.concatenate((a,d),axis=0) #Concatenate arrays\n\nnp.vstack((a,b) #Stack arrays vertically(row wise)\n\nnp.r_[e,f] #Stack arrays vertically(row wise)\nnp.hstack((e,f)) #Stack arrays horizontally(column wise)\n\nnp.column_stack((a,d)) #Create stacked column wise arrays\n\nnp.c_[a,d] #Create stacked column wise arrays\n\n\nnp.hsplit(a,3) #Split the array horizontally at the 3rd index\n\nnp.vsplit(c,2) #Split the array vertically at the 2nd index"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-pandas-introduction",
    "href": "MsaPythonDataAnalytics.html#python-pandas-introduction",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python Pandas | Introduction",
    "text": "Python Pandas | Introduction\nPandas : Python module/library enhancing data manipulation tools designed to make data cleaning and analysis fast and convenient in Python. Works intandem with numpy and is the core working under the hood.\n\nIntroPandas SummaryPandas Import/Export [Read/Write]Pandas Data CleaningDate & TimeJoins & MergeLet us get our hands dirtyBonus material\n\n\n\n\nCode\nimport pandas as pd\n\nobj_series = pd.Series([4, 7, -5, 3])\n\nobj_series\n\n\n0    4\n1    7\n2   -5\n3    3\ndtype: int64\n\n\nMain difference of pandas with numpy is that it has indexed values and designed for working with tabular or heterogeneous data.\nPandas relies on dataframes this is excel like data format with rows/records and columns/fields. Which mankes it easy to work with.\nEach row stands for an observation and columns here are variables.\n\n\nCode\ndata = {\n  'county' : ['Nairobi','Kiambu','Kajiado','Machakos'],\n  'headquarters' : ['Nairobi','Kiambu','Kajiado','Machakos'],\n  'population' : [4397073,2417735,1117840,1421932]\n}\n\ndf_data = pd.DataFrame(data,columns=['county','headquarters','population'])\ndf_data\n\n\n\n\n\n\n\n\n\ncounty\nheadquarters\npopulation\n\n\n\n\n0\nNairobi\nNairobi\n4397073\n\n\n1\nKiambu\nKiambu\n2417735\n\n\n2\nKajiado\nKajiado\n1117840\n\n\n3\nMachakos\nMachakos\n1421932\n\n\n\n\n\n\n\nChecking and investigating the dataframe\n\n\nCode\ndf_data.shape()\ndf_data.index()\ndf_data.columns()\ndf_data.info()\ndf_data.count()\n\n\n\n\n\n\nCode\ndf_data.sum()\ndf_data.cumsum()\ndf_data.min()\ndf_data.max()\ndf_data.idmax()\ndf_data.idmin()\ndf_data.describe()\ndf_data.mean()\ndf_data.media()\n\n\n\n\nRead CSV\n\n\nCode\n#import pandas as pd\ndf = pd.read_csv('file.csv', header = None, nrows=5)\ndf.to_csv(\"first_dataframe.csv\")\n\n\nRead excel\n\n\nCode\ndf = pd.read_excel('excel_file.xlsx', sheet = 'Sheet1')\n\ndf_mulitple_excel = pd.ExcelFile('excel_file.xlsx')\ndf = pd.read_excel(df_mulitple_excel,  'Sheet1')\n\n\ndf.to_excel('first_dataframe.xlsx', sheet_name = 'Sheet first')\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nread_csv\nLoad delimited data from a file, URL, or file-like object; use comma as default delimiter\n\n\nread_fwf\nRead data in fixed-width column format (i.e., no delimiters)\n\n\nread_clipboard\nVariation of read_csv that reads data from the clipboard; useful for converting tables from web pages\n\n\nread_excel\nRead tabular data from an Excel XLS or XLSX file\n\n\nread_hdf\nRead HDF5 files written by pandas\n\n\nread_html\nRead all tables found in the given HTML document\n\n\nread_json\nRead data from a JSON (JavaScript Object Notation) string representation, file, URL, or file-like object\n\n\nread_feather\nRead the Feather binary file format\n\n\nread_orc\nRead the Apache ORC binary file format\n\n\nread_parquet\nRead the Apache Parquet binary file format\n\n\nread_pickle\nRead an object stored by pandas using the Python pickle format\n\n\nread_sas\nRead a SAS dataset stored in one of the SAS system’s custom storage formats\n\n\nread_spss\nRead a data file created by SPSS\n\n\nread_sql\nRead the results of a SQL query (using SQLAlchemy)\n\n\nread_sql_table\nRead a whole SQL table (using SQLAlchemy); equivalent to using a query that selects everything in that table using read_sql\n\n\nread_stata\nRead a dataset from Stata file format\n\n\nread_xml\nRead a table of data from an XML file\n\n\n\n\n\n\n80% of the work done on data is cleaning\n\nDealing with missing data\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ndropna\nFilter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n\n\nfillna\nFill in missing data with some value or using an interpolation method such as “ffill” or “bfill”.\n\n\nisna\nReturn Boolean values indicating which values are missing/NA.\n\n\nnotna\nNegation of isna, returns True for non-NA values and False for NA values.\n\n\n\nData Transformation\nRemoving Duplicates\n\n\nCode\ndata = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n                     \"k2\": [1, 1, 2, 3, 3, 4, 4]})\n                     \ndata\n\ndata.duplicated()\n\ndata.drop_duplicates()\n\ndata[\"v1\"] = range(7)\n\ndata\n\ndata.drop_duplicates(subset=[\"k1\"])\n\ndata.drop_duplicates([\"k1\", \"k2\"], keep=\"last\")\n\ndata.sort_values(by = 'k1') #sort\n\n\nSample Cleaning\n\n\nCode\n# Reading data using pandas\ndf = pd.read_csv(\"https://rcs.bu.edu/examples/python/DataAnalysis/Salaries.csv\")\n# List first 5 records\ndf.head()\n\n#Select column\n\ndf['sex']\ndf.sex\n\n# #Group data using rank\ndf_rank = df.groupby([\"rank\"])\n\ndf_rank.head()\n\n# #Calculate mean value for each numeric column per each group\ndf_rank.mean()\n\n# Once groupby object is created we can calculate various statistics for each group:\n\n#Calculate mean salary for each professor rank:\ndf.groupby('rank')[['salary']].mean()\n\n# Note: If single brackets are used to specify the column (e.g. salary), then the output is Pandas Series object.\n# When double brackets are used the output is a Data Frame\n\n\n#Calculate mean salary for each professor rank:\ndf.groupby(['rank'], sort=False)[['salary']].mean()\n\n\n# subset the rows in which the salary value is greater than $120K: \ndf_sub = df[df['salary'] &gt; 120000]\ndf_sub.head()\n\n#Select only those rows that contain female professors:\ndf_f = df[df['sex'] == 'Female']\n\n#Selecting rows\ndf[0:10]\n\n#Select rows by their labels:\ndf_sub.loc[10:20,['rank','sex','salary']]\n\n\n#Select rows by their labels:\ndf_sub.iloc[10:20,[0, 3, 4, 5]]\n\n\n#We can sort the data using 2 or more columns:\ndf_sorted = df.sort_values( by =['service', 'salary'], ascending = [True, False])\ndf_sorted.head(10)\n\n\nSample 2*\n\n\nCode\n# Read a dataset with missing values\nflights = pd.read_csv(\"https://rcs.bu.edu/examples/python/DataAnalysis/flights.csv\")\n\n# Select the rows that have at least one missing value\nflights[flights.isnull().any(axis=1)].head()\n\n\n#There are a number of methods to deal with missing values in the data frame:\n   \n#df.dropna(): drop missing observations\n#df.dropna(how = \"all\"):  drop observtions where all cells is NA\n#df.dropna(axis = 1,how = \"all\"): drop column if all the values are missing\n#df.dropna(thresh = 5): Drop rows that contain less than 5 non-missing values\n#df.fillna(0): Replace missing values with zeros\n#df.isnull(): returns True if the value is missing\n#df.notnull(): Returns True for non-missing values\n\n\nAggregation Functions in Pandas\nAggregation - computing a summary statistic about each group, i.e. compute group sums or means compute group sizes/counts\nCommon aggregation functions:\nmin, max count, sum, prod mean, median, mode, mad std, var\n\n\nCode\nflights[['dep_delay','arr_delay']].agg(['min','mean','max'])\n\nflights.groupby('origin').count()\n\nflights['origin'].value_counts()\n\n\n\n\n\nDate: Handles dates without time.\nPOSIXct: Handles date & time in calendar time.\nPOSIXlt: Handles date & time in local time.\nHms: Parses periods with hour, minute, and second\nTimestamp: Represents a single pandas date & time\nInterval: Defines an open or closed range between dates and times\nTime delta: Computes time difference between different datetimes\n\nISO8601 datetime format (YYYY-MM-DD HH:MM:SS TZ)\n\n\nCode\nimport datetime as dt\nimport time as tm\nimport pytz\nimport pandas as pd\n\n\n\n\nCode\n# Get the current date\ndt.date.today()\n\n# Get the current date and time\ndt.datetime.now()\n\n\ndatetime.datetime(2024, 11, 20, 23, 18, 31, 754285)\n\n\nParsing dates, datetimes, and times\n\n\nCode\niso = pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\n\nus = ['07/20/1969 20:17:40','11/19/1969 06:54:35']\n\nnon_us = ['20/07/1969 20:17:40','19/11/1969 06:54:35']\n\nparts = pd.DataFrame({\n  'year' : [1969,1969,1971],\n  'month' : [7,11,2],\n  'day' : [20,19,5]\n})\n\n# Parse dates in ISO format\npd.to_datetime(iso)\n\n# Parse dates in US format\npd.to_datetime(us, dayfirst=False)\n\n# Parse dates in NON US format\npd.to_datetime(non_us, dayfirst=True)\n\n# Parse dates, guessing a single format\npd.to_datetime(iso, infer_datetime_format=True)\n\n# Parse dates in single, specified format\npd.to_datetime(iso, format=\"%Y-%m-%d %H:%M:%S\")\n\n# Parse dates in single, specified format\npd.to_datetime(us, format=\"%m/%d/%Y %H:%M:%S\")\n\n# Make dates from components\npd.to_datetime(parts)\n\n\n0   1969-07-20\n1   1969-11-19\n2   1971-02-05\ndtype: datetime64[ns]\n\n\nExtracting components\n\n\nCode\n# Parse strings to datetimes\ndttm = pd.to_datetime(iso)\n\n# Get year from datetime pandas series\ndttm.year\n\n# Get day of the year from datetime pandas series\ndttm.day_of_year\n\n# Get month name from datetime pandas series\ndttm.month_name()\n\n# Get day name from datetime pandas series\ndttm.day_name()\n\n# Get datetime.datetime format from datetime pandas series\ndttm.to_pydatetime()\n\n\narray([datetime.datetime(2018, 10, 26, 12, 0, tzinfo=datetime.timezone.utc),\n       datetime.datetime(2018, 10, 26, 13, 0, tzinfo=datetime.timezone.utc)],\n      dtype=object)\n\n\nRounding Dates\n\n\nCode\n# Rounding dates to nearest time unit\ndttm.round('1min')\n\n# Flooring dates to nearest time unit\ndttm.floor('1min')\n\n# Ceiling dates to nearest time unit\ndttm.ceil('1min')\n\n\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)\n\n\nArithmetic\n\n\nCode\n# Create two datetimes\nnow = dt.datetime.now()\nthen = pd.Timestamp('2021-09-15 10:03:30')\n\n# Get time elapsed as timedelta object\nnow - then\n\n# Get time elapsed in seconds \n(now - then).total_seconds()\n\n# Adding a day to a datetime\ndt.datetime(2022,8,5,11,13,50) + dt.timedelta(days=1)\n\n\ndatetime.datetime(2022, 8, 6, 11, 13, 50)\n\n\nTime Intervals\n\n\nCode\n# Create interval datetimes\nstart_1 = pd.Timestamp('2021-10-21 03:02:10')\nfinish_1 = pd.Timestamp('2022-09-15 10:03:30')\nstart_2 = pd.Timestamp('2022-08-21 03:02:10')\nfinish_2 = pd.Timestamp('2022-12-15 10:03:30')\n\n# Specify the interval between two datetimes\npd.Interval(start_1, finish_1, closed='right')\n\n# Get the length of an interval\npd.Interval(start_1, finish_1, closed='right').length\n\n# Determine if two intervals are intersecting\npd.Interval(start_1, finish_1, closed='right').overlaps(pd.Interval(start_2, finish_2, closed='right'))\n\n\nTrue\n\n\nTime Deltas\n\n\nCode\n# Define a timedelta in days\npd.Timedelta(7, \"d\")\n\n# Convert timedelta to seconds\npd.Timedelta(7, \"d\").total_seconds()\n\n\n604800.0\n\n\n\n\nMerge*\n\n\nCode\ndf1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\ndf2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n                    'hire_date': [2004, 2008, 2012, 2014]})\n\ndf3 = pd.merge(df1, df2)\ndf3\n\n\ndf4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n                    'supervisor': ['Carly', 'Guido', 'Steve']})\n                    \npd.merge(df3, df4)\n\n#Many-to-many joins\n\ndf5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n                              'Engineering', 'Engineering', 'HR', 'HR'],\n                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n                               'spreadsheets', 'organization']})\n                               \npd.merge(df1, df5)\n\n\ndf3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n                    'salary': [70000, 80000, 120000, 90000]})\n                    \npd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")\n\n#Drop redundant column\npd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)\n\n\nJoin\n\n\nCode\ndf1a = df1.set_index('employee')\ndf2a = df2.set_index('employee')\n\npd.merge(df1a, df2a, left_index=True, right_index=True)\n\n\n#For convenience, DataFrames implement the join() method, which performs a merge that defaults to joining on indices:\n\ndf1a.join(df2a)\n\n\n\ndf6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n                    'food': ['fish', 'beans', 'bread']},\n                   columns=['name', 'food'])\ndf7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n                    'drink': ['wine', 'beer']},\n                   columns=['name', 'drink'])\n                   \n                   \npd.merge(df6, df7)\n\n#Inner Join\n\npd.merge(df6, df7, how='inner')\n\n#Other options for the how keyword are 'outer', 'left', and 'right'. An outer join returns a join over the union of the input columns, and fills in all missing values with NAs\n\npd.merge(df6, df7, how='outer')\n\n\npd.merge(df6, df7, how='left')\n\n\n\n\nFrom the google drive link request for access and download the data google drive\n\nDownload the state data\nMerge the three different csv’s\nRank the population density for the different states in 2010\n\n\npopulation density calculation = population size / state size area\n\n\nTop 3 states\n\n\n\nCollapsing data into categories Map categories to fewer ones: reducing categories in categorical column.\n\n\nCode\n# Create mapping dictionary and replace\nmapping = {'Microsoft':'ComputerOS', 'MacOS':'ComputerOS', 'Linux':'ComputerOS',\n'IOS':'MobileOS', 'Android':'MobileOS'}\ndevices['operating_system'] = devices['operating_system'].replace(mapping)\ndevices['operating_system'].unique()\n\n\n# Replace \"+\" with \"00\"\nphones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"+\", \"00\")\nphones\n\n# Replace \"-\" with nothing\nphones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"-\", \"\")\nphones\n\n\n# Replace phone numbers with lower than 10 digits to NaN\ndigits = phones['Phone number'].str.len()\nphones.loc[digits &lt; 10, \"Phone number\"] = np.nan\nphones\n\n\n# Replace letters with nothing\nphones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')\nphones.head()\n\n\n# Treating date data\nbirthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")\nbirthdays.head()\n\n\nsum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis = 1)\npassenger_equ = sum_classes == flights['total_passengers']\n# Find and filter out rows with inconsistent passengers\ninconsistent_pass = flights[~passenger_equ]\nconsistent_pass = flights[passenger_equ]\n\n\n# Drop missing values\nairquality_dropped = airquality.dropna(subset = ['CO2'])\nairquality_dropped.head()\n\n#Replacing with statistical measures\nco2_mean = airquality['CO2'].mean()\nairquality_imputed = airquality.fillna({'CO2': co2_mean})\nairquality_imputed.head()\n\n\n# Multiple grouped summaries\ndogs.groupby(\"color\")[\"weight_kg\"].agg([min, max, sum])\n\n# Grouping by multiple variables\ndogs.groupby([\"color\", \"breed\"])[\"weight_kg\"].mean()\n\ndogs.groupby([\"color\", \"breed\"])[[\"weight_kg\", \"height_cm\"]].mean()\n\n\n\n# Group by to pivot table\ndogs.pivot_table(values=\"weight_kg\", index=\"color\")\n\n\n# Different statistics\nimport numpy as np\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", aggfunc=np.median)\n\n# Multiple statistics\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", aggfunc=[np.mean, np.median])\n\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\")\n\n# Filling missing values in pivot tables\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\", \nfill_value=0)\n\n# Summing with pivot tables\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\",\nfill_value=0, margins=True)"
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#visualization",
    "href": "MsaPythonDataAnalytics.html#visualization",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Visualization",
    "text": "Visualization\nThe point of visualization is being able to summary data to palatable chunks, easy to read and digest. Quick glances and be able to tell a story.\nHumans are visual, absorbing more through image than any other thing. There are very many examples of visuals that have impacted the world I will talk about 3.\n\nThe general minrad maps\nThe nurse crimea wars\nThe Doctor cholera maps\n\nAny visual should be able to tell a story either for record purposes and future solutions, predictions."
  },
  {
    "objectID": "MsaPythonDataAnalytics.html#python-visualization-matplotlip-seaborn",
    "href": "MsaPythonDataAnalytics.html#python-visualization-matplotlip-seaborn",
    "title": "Data Analytics Introduction Using Python Training",
    "section": "Python Visualization | Matplotlip & Seaborn",
    "text": "Python Visualization | Matplotlip & Seaborn\nPython provides so many modules for visualization but for purposes of the training we will deal with the two most common visualiation tools.\n\nmatplotlib\nseaborn\nplotnine\naltair\nBokeh\n\n\nMatplotlipSeabornPlotninePandas Inbuilt Visualization\n\n\nMatplotlib is the “grandfather” library of data visualization with Python. It was created by John Hunter. He created it to try to replicate MatLab’s (another programming language) plotting capabilities in Python. So if you happen to be familiar with matlab, matplotlib will feel natural to you.\nIt is an excellent 2D and 3D graphics library for generating scientific figures.\nSome of the major Pros of Matplotlib are:\n\nGenerally easy to get started for simple plots\nSupport for custom labels and texts\nGreat control of every element in a figure\nHigh-quality output in many formats\nVery customizable in general\n\nMatplotlib allows you to create reproducible figures programmatically. Let’s learn how to use it! Before continuing this lecture, I encourage you just to explore the official Matplotlib web page: http://matplotlib.org/\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nimport numpy as np\nx = np.linspace(0, 5, 11)\ny = x ** 2\n\n\nSince we are using ipython notebook we need to run %matplotlib inline before running the plot\n\n\nCode\nplt.plot(x, y, 'r') # 'r' is the color red\nplt.xlabel('X Axis Title Here')\nplt.ylabel('Y Axis Title Here')\nplt.title('String Title Here')\nplt.show()\n\n\n\n\n\nBar charts\n\n\nCode\nx = [10, 20, 30, 40, 50, 60]\ny = [13, 45, 23, 34, 96, 76]\nplt.title('Bar Graph')\nplt.bar(x, y, color='dodgerblue', width=5)\nplt.show()\n\n\n\n\n\nPie charts\n\n\nCode\nx = [35, 20, 30, 40, 50, 30]\ny = ['Apple', 'Bananna', 'Grapes', 'Orange', 'PineApple', 'Dragon Fruit']\nplt.title('Pie Chart')\nplt.pie(x, labels=y)\nplt.show()\n\n\n\n\n\nBox plots\n\n\nCode\n# Sample data\ndata = {\n    'Category': ['A']*10 + ['B']*10,\n    'Value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n}\n\ndf = pd.DataFrame(data)\n\n# Box plot\ndf.boxplot(by='Category')\nplt.title('Box Plot Example')\nplt.suptitle('')\nplt.xlabel('Category')\nplt.ylabel('Value')\nplt.show()\n\n\n\n\n\n\n\nSeaborn module for visualization\nLine Graph\n\n\nCode\n# importing packages\nimport seaborn as sns\n# loading dataset\ndata = sns.load_dataset(\"iris\")\n# draw lineplot\nsns.lineplot(x=\"sepal_length\", y=\"sepal_width\", data=data)\n\n\n&lt;Axes: xlabel='sepal_length', ylabel='sepal_width'&gt;\n\n\n\n\n\nScatter Graph\n\n\nCode\ndata = sns.load_dataset(\"iris\")\nsns.scatterplot(data=data)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\n#import seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips)\n\n\n&lt;Axes: xlabel='total_bill', ylabel='tip'&gt;\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\n\n\ntips = sns.load_dataset(\"tips\")\n\n\n\n\n# customize the scatter plot\n\nsns.scatterplot(x=\"total_bill\", y=\"tip\", hue=\"sex\", size=\"size\", sizes=(50, 200), data=tips)\n\n\n\n\n# add labels and title\n\nplt.xlabel(\"Total Bill\")\n\nplt.ylabel(\"Tip\")\n\nplt.title(\"Relationship between Total Bill and Tip\")\n\n\n\n\n# display the plot\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.plot.area\n\ndf.plot.barh\n\ndf.plot.density\n\ndf.plot.hist\n\ndf.plot.line\n\ndf.plot.scatter\ndf.plot.bar\n\ndf.plot.box\n\ndf.plot.hexbin\n\ndf.plot.kde\n\ndf.plot.pie"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html",
    "href": "samples/Pandas Built-in Data Visualization.html",
    "title": "Pandas Built-in Data Visualization",
    "section": "",
    "text": "In this lecture we will learn about pandas built-in capabilities for data visualization! It’s built-off of matplotlib, but it baked into pandas for easier usage!\nLet’s take a look!"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#imports",
    "href": "samples/Pandas Built-in Data Visualization.html#imports",
    "title": "Pandas Built-in Data Visualization",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#the-data",
    "href": "samples/Pandas Built-in Data Visualization.html#the-data",
    "title": "Pandas Built-in Data Visualization",
    "section": "The Data",
    "text": "The Data\nThere are some fake data csv files you can read in as dataframes:\n\npd.read\n\n\ndf1 = pd.read_csv('df1',index_col=0)\ndf2 = pd.read_csv('df2')"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#style-sheets",
    "href": "samples/Pandas Built-in Data Visualization.html#style-sheets",
    "title": "Pandas Built-in Data Visualization",
    "section": "Style Sheets",
    "text": "Style Sheets\nMatplotlib has style sheets you can use to make your plots look a little nicer. These style sheets include plot_bmh,plot_fivethirtyeight,plot_ggplot and more. They basically create a set of style rules that your plots follow. I recommend using them, they make all your plots have the same look and feel more professional. You can even create your own if you want your company’s plots to all have the same look (it is a bit tedious to create on though).\nHere is how to use them.\nBefore plt.style.use() your plots look like this:\n\ndf1['A'].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x125853940&gt;\n\n\n\n\n\nCall the style:\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nNow your plots look like this:\n\ndf1['A'].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x12588d358&gt;\n\n\n\n\n\n\nplt.style.use('bmh')\ndf1['A'].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x125bec080&gt;\n\n\n\n\n\n\nplt.style.use('dark_background')\ndf1['A'].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1259eb780&gt;\n\n\n\n\n\n\nplt.style.use('fivethirtyeight')\ndf1['A'].hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x125fad2b0&gt;\n\n\n\n\n\n\nplt.style.use('ggplot')\n\nLet’s stick with the ggplot style and actually show you how to utilize pandas built-in plotting capabilities!"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#area",
    "href": "samples/Pandas Built-in Data Visualization.html#area",
    "title": "Pandas Built-in Data Visualization",
    "section": "Area",
    "text": "Area\n\ndf2.plot.area(alpha=0.4)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x126222978&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#barplots",
    "href": "samples/Pandas Built-in Data Visualization.html#barplots",
    "title": "Pandas Built-in Data Visualization",
    "section": "Barplots",
    "text": "Barplots\n\ndf2.head()\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.039762\n0.218517\n0.103423\n0.957904\n\n\n1\n0.937288\n0.041567\n0.899125\n0.977680\n\n\n2\n0.780504\n0.008948\n0.557808\n0.797510\n\n\n3\n0.672717\n0.247870\n0.264071\n0.444358\n\n\n4\n0.053829\n0.520124\n0.552264\n0.190008\n\n\n\n\n\n\n\n\ndf2.plot.bar()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x126388630&gt;\n\n\n\n\n\n\ndf2.plot.bar(stacked=True)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x12657bb38&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#histograms",
    "href": "samples/Pandas Built-in Data Visualization.html#histograms",
    "title": "Pandas Built-in Data Visualization",
    "section": "Histograms",
    "text": "Histograms\n\ndf1['A'].plot.hist(bins=50)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x12685e9b0&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#line-plots",
    "href": "samples/Pandas Built-in Data Visualization.html#line-plots",
    "title": "Pandas Built-in Data Visualization",
    "section": "Line Plots",
    "text": "Line Plots\n\ndf1.plot.line(x=df1.index,y='B',figsize=(12,3),lw=1)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1243875f8&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#scatter-plots",
    "href": "samples/Pandas Built-in Data Visualization.html#scatter-plots",
    "title": "Pandas Built-in Data Visualization",
    "section": "Scatter Plots",
    "text": "Scatter Plots\n\ndf1.plot.scatter(x='A',y='B')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x126b90fd0&gt;\n\n\n\n\n\nYou can use c to color based off another column value Use cmap to indicate colormap to use. For all the colormaps, check out: http://matplotlib.org/users/colormaps.html\n\ndf1.plot.scatter(x='A',y='B',c='C',cmap='coolwarm')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x126f7b400&gt;\n\n\n\n\n\nOr use s to indicate size based off another column. s parameter needs to be an array, not just the name of a column:\n\ndf1.plot.scatter(x='A',y='B',s=df1['C']*200)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x126f94c18&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#boxplots",
    "href": "samples/Pandas Built-in Data Visualization.html#boxplots",
    "title": "Pandas Built-in Data Visualization",
    "section": "BoxPlots",
    "text": "BoxPlots\n\ndf2.plot.box() # Can also pass a by= argument for groupby\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1271fa198&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#hexagonal-bin-plot",
    "href": "samples/Pandas Built-in Data Visualization.html#hexagonal-bin-plot",
    "title": "Pandas Built-in Data Visualization",
    "section": "Hexagonal Bin Plot",
    "text": "Hexagonal Bin Plot\nUseful for Bivariate Data, alternative to scatterplot:\n\ndf = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b'])\ndf.plot.hexbin(x='a',y='b',gridsize=25,cmap='Oranges')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x127413358&gt;"
  },
  {
    "objectID": "samples/Pandas Built-in Data Visualization.html#kernel-density-estimation-plot-kde",
    "href": "samples/Pandas Built-in Data Visualization.html#kernel-density-estimation-plot-kde",
    "title": "Pandas Built-in Data Visualization",
    "section": "Kernel Density Estimation plot (KDE)",
    "text": "Kernel Density Estimation plot (KDE)\n\ndf2['a'].plot.kde()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1276d9160&gt;\n\n\n\n\n\n\ndf2.plot.density()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x1276f7940&gt;\n\n\n\n\n\nThat’s it! Hopefully you can see why this method of plotting will be a lot easier to use than full-on matplotlib, it balances ease of use with control over the figure. A lot of the plot calls also accept additional arguments of their parent matplotlib plt. call.\nNext we will learn about seaborn, which is a statistical visualization library designed to work with pandas dataframes well.\nBefore that though, we’ll have a quick exercise for you!"
  }
]